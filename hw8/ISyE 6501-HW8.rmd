---
title: "ISyE 6501-HOMEWORK 8"
output:
  pdf_document:
    extra_dependencies: ["xcolor"]
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(prompt = TRUE)
        knitr::opts_chunk$set(comment = NA)
        knitr::opts_chunk$set(fig.height = 5)
        knitr::opts_chunk$set(fig.width = 7)
        knitr::opts_chunk$set(fig.align = "center")
```
  
**Group Members:**  
Jinyu, Huang  |  jhuang472@gatech.edu  |  GTID: 903522245  
Chengqi, Huang  |  chengqihuang@gatech.edu  |  GTID: 903534690  
Yuefan, Hu  |  yuefanhu@gatech.edu  |  GTID:903543027  
Jingyu, Li  |  alanli@gatech.edu  |  GTID: 903520148  
  
  
# Qusetion 11.1  
## Stepwise regression  
As what our group has explored in Homework 5, we choose AICc as the metric for us to choose model considering the small sample size. We used a modified version of *stepAIC* from library "MASS" called *stepAICc*(https://stat.ethz.ch/pipermail/r-help/2009-April/389888.html). We use the full model (including all predictors) as initial model.
```{r}
crime = read.table("uscrime.txt", header=TRUE)  # import data
crime$So = as.factor(crime$So)
```
```{r, include=FALSE}
library(MuMIn)
stepAICc <- function (object, scope, scale = 0, direction = c("both", "backward", 
  "forward"), trace = 1, keep = NULL, steps = 1000, use.start = FALSE, 
  k = 2, ...) 
{
  mydeviance <- function(x, ...) {
    dev <- deviance(x)
    if (!is.null(dev)) 
      dev
    else MuMIn::AICc(x, k=0)
  }
  cut.string <- function(string) {
    if (length(string) > 1L) 
      string[-1L] <- paste("\n", string[-1L], sep = "")
    string
  }
  re.arrange <- function(keep) {
    namr <- names(k1 <- keep[[1L]])
    namc <- names(keep)
    nc <- length(keep)
    nr <- length(k1)
    array(unlist(keep, recursive = FALSE), c(nr, nc), list(namr, 
      namc))
  }
  step.results <- function(models, fit, object, usingCp = FALSE) {
    change <- sapply(models, "[[", "change")
    rd <- sapply(models, "[[", "deviance")
    dd <- c(NA, abs(diff(rd)))
    rdf <- sapply(models, "[[", "df.resid")
    ddf <- c(NA, abs(diff(rdf)))
    AIC <- sapply(models, "[[", "AIC")
    heading <- c("Stepwise Model Path \nAnalysis of Deviance Table", 
      "\nInitial Model:", deparse(formula(object)), "\nFinal Model:", 
      deparse(formula(fit)), "\n")
    aod <- if (usingCp) 
      data.frame(Step = change, Df = ddf, Deviance = dd, 
        `Resid. Df` = rdf, `Resid. Dev` = rd, Cp = AIC, 
        check.names = FALSE)
    else data.frame(Step = change, Df = ddf, Deviance = dd, 
      `Resid. Df` = rdf, `Resid. Dev` = rd, AIC = AIC, 
      check.names = FALSE)
    attr(aod, "heading") <- heading
    class(aod) <- c("Anova", "data.frame")
    fit$anova <- aod
    fit
  }
  Terms <- terms(object)
  object$formula <- Terms
  if (inherits(object, "lme")) 
    object$call$fixed <- Terms
  else if (inherits(object, "gls")) 
    object$call$model <- Terms
  else object$call$formula <- Terms
  if (use.start) 
    warning("'use.start' cannot be used with R's version of 'glm'")
  md <- missing(direction)
  direction <- match.arg(direction)
  backward <- direction == "both" | direction == "backward"
  forward <- direction == "both" | direction == "forward"
  if (missing(scope)) {
    fdrop <- numeric()
    fadd <- attr(Terms, "factors")
    if (md) 
      forward <- FALSE
  }
  else {
    if (is.list(scope)) {
      fdrop <- if (!is.null(fdrop <- scope$lower)) 
        attr(terms(update.formula(object, fdrop)), "factors")
      else numeric()
      fadd <- if (!is.null(fadd <- scope$upper)) 
        attr(terms(update.formula(object, fadd)), "factors")
    }
    else {
      fadd <- if (!is.null(fadd <- scope)) 
        attr(terms(update.formula(object, scope)), "factors")
      fdrop <- numeric()
    }
  }
  models <- vector("list", steps)
  if (!is.null(keep)) 
    keep.list <- vector("list", steps)
  n <- nobs(object, use.fallback = TRUE)
  fit <- object
  bAIC <- extractAIC(fit, scale, k = k, ...)
  edf <- bAIC[1L]
  bAIC <- MuMIn::AICc(fit, k=k)
  if (is.na(bAIC)) 
    stop("AIC is not defined for this model, so 'stepAIC' cannot proceed")
  if (bAIC == -Inf) 
    stop("AIC is -infinity for this model, so 'stepAIC' cannot proceed")
  nm <- 1
  Terms <- terms(fit)
  if (trace) {
    cat("Start:  AIC=", format(round(bAIC, 2)), "\n", cut.string(deparse(formula(fit))), 
      "\n\n", sep = "")
    utils::flush.console()
  }
  models[[nm]] <- list(deviance = mydeviance(fit), df.resid = n - 
    edf, change = "", AIC = bAIC)
  if (!is.null(keep)) 
    keep.list[[nm]] <- keep(fit, bAIC)
  usingCp <- FALSE
  while (steps > 0) {
    steps <- steps - 1
    AIC <- bAIC
    ffac <- attr(Terms, "factors")
    if (!is.null(sp <- attr(Terms, "specials")) && !is.null(st <- sp$strata)) 
      ffac <- ffac[-st, ]
    scope <- factor.scope(ffac, list(add = fadd, drop = fdrop))
    aod <- NULL
    change <- NULL
    if (backward && length(scope$drop)) {
      aod <- dropterm(fit, scope$drop, scale = scale, 
        trace = max(0, trace - 1), k = k, ...)
      rn <- row.names(aod)
      row.names(aod) <- c(rn[1L], paste("-", rn[-1L], 
        sep = " "))
      if (any(aod$Df == 0, na.rm = TRUE)) {
        zdf <- aod$Df == 0 & !is.na(aod$Df)
        nc <- match(c("Cp", "AIC"), names(aod))
        nc <- nc[!is.na(nc)][1L]
        ch <- abs(aod[zdf, nc] - aod[1, nc]) > 0.01
        if (any(is.finite(ch) & ch)) {
          warning("0 df terms are changing AIC")
          zdf <- zdf[!ch]
        }
        if (length(zdf) > 0L) 
          change <- rev(rownames(aod)[zdf])[1L]
      }
    }
    if (is.null(change)) {
      if (forward && length(scope$add)) {
        aodf <- addterm(fit, scope$add, scale = scale, 
          trace = max(0, trace - 1), k = k, ...)
        rn <- row.names(aodf)
        row.names(aodf) <- c(rn[1L], paste("+", rn[-1L], 
          sep = " "))
        aod <- if (is.null(aod)) 
          aodf
        else rbind(aod, aodf[-1, , drop = FALSE])
      }
      attr(aod, "heading") <- NULL
      if (is.null(aod) || ncol(aod) == 0) 
        break
      nzdf <- if (!is.null(aod$Df)) 
        aod$Df != 0 | is.na(aod$Df)
      aod <- aod[nzdf, ]
      if (is.null(aod) || ncol(aod) == 0) 
        break
      nc <- match(c("Cp", "AIC"), names(aod))
      nc <- nc[!is.na(nc)][1L]
      o <- order(aod[, nc])
      if (trace) {
        print(aod[o, ])
        utils::flush.console()
      }
      if (o[1L] == 1) 
        break
      change <- rownames(aod)[o[1L]]
    }
    usingCp <- match("Cp", names(aod), 0) > 0
    fit <- update(fit, paste("~ .", change), evaluate = FALSE)
    fit <- eval.parent(fit)
    nnew <- nobs(fit, use.fallback = TRUE)
    if (all(is.finite(c(n, nnew))) && nnew != n) 
      stop("number of rows in use has changed: remove missing values?")
    Terms <- terms(fit)
    bAIC <- extractAIC(fit, scale, k = k, ...)
    edf <- bAIC[1L]
    bAIC <- MuMIn::AICc(fit, k=k)
    if (trace) {
      cat("\nStep:  AIC=", format(round(bAIC, 2)), "\n", 
        cut.string(deparse(formula(fit))), "\n\n", sep = "")
      utils::flush.console()
    }
    if (bAIC >= AIC + 1e-07) 
      break
    nm <- nm + 1
    models[[nm]] <- list(deviance = mydeviance(fit), df.resid = n - 
      edf, change = change, AIC = bAIC)
    if (!is.null(keep)) 
      keep.list[[nm]] <- keep(fit, bAIC)
  }
  if (!is.null(keep)) 
    fit$keep <- re.arrange(keep.list[seq(nm)])
  step.results(models = models[seq(nm)], fit, object, usingCp)
}
```
```{r}
library(MASS)
full.model = lm(Crime~., data=crime)
stepAICc(full.model, direction = "both", steps=2000) 
```
  
The stepwise method showes that model Crime ~ M + Ed + Po1 + M.F + U1 + U2 + Ineq + Prob has the lowest AICc. In the model, the coefficients of M, ED, PO1, U2, Ineq and Prob are significant while U1's coefficient is marginally significant. The adjusted R-squared of the model is 0.744, which is relatively high. 
```{r}
model_stepwise = lm(Crime~M+Ed+Po1+M.F+U1+U2+Ineq+Prob, data=crime)
summary(model_stepwise)
```
  
  
## Lasso  
According to the document of *glmnet* function, the algorithm is to minimize the objective function: $errors+\lambda\sum\|\beta_i\|$, which is not exactly the same approach as what's discussed in the lecture. Instead of setting the lasso constraint $\sum\|\beta_i\|\leq\tau$, running *glmnet* function will generate different values of $\lambda$ and then find $\beta_i$ that can minimize the objective function $errors+\lambda\sum\|\beta_i\|$. As we can see from the figures below, when the value of $\lambda$ becomes larger, the number of non-zero predictor coefficients decreases and the fraction of deviance explained decreases too. This makes sense because when $\lambda$ gets larger, the influence of penalty term $\sum\|\beta_i\|$ in the objective function $errors+\lambda\sum\|\beta_i\|$ becomes greater. In order to minimize the objective, $\sum\|\beta_i\|$ will get closer to zero so that more coefficients of predictors will equal to zero and less predictors are included in the model. The mechanism of increasing $\lambda$ is the same as decreasing $\tau$ in the constraint.
```{r}
library(glmnet)
library(Matrix)
library(foreach)
model_lasso = glmnet(as.matrix(crime[,-16]), as.matrix(crime[,16]), 
                     family="mgaussian", alpha=1,
                     standardize=TRUE, standardize.response=TRUE)
par(mfrow=c(1,2))
plot(model_lasso$lambda, model_lasso$df, ylab="num of non-zero coefficient")
plot(model_lasso$lambda, model_lasso$dev.ratio, ylab="fraction of deviance explained")
```
  
  
Because there is no way for us to do validation or testing based on this small sample. The way helping us to select value of $\lambda$ is to compare metrics like AICc and adjusted R-squared comprehensively. Considering the tradeoff between model complexity(number of predictors) and model fitting(fraction of deviance explained), we propose that we set $\lambda=0.025, 0.05, 0.075, 0.1$ as example and the results showed below (if needed, we can traverse all the lambdas we get). Among these models, we tend to choose Crime~M+Ed+Po1+M.F+NW+Ineq+Prob.
```{r}
model_lasso_new = glmnet(as.matrix(crime[,-16]), as.matrix(crime[,16]), 
                     family="mgaussian", alpha=1, lambda=c(0.025,0.05,0.075,0.1),
                     standardize=TRUE, standardize.response=TRUE)
model_lasso_new$beta
cat("lambda=0.1\n")
model = lm(Crime~M+Po1+M.F+Ineq+Prob, data=crime)
AICc(model)
summary(model)$adj.r.squared
cat("lambda=0.075\n")
model = lm(Crime~M+Ed+Po1+M.F+NW+Ineq+Prob, data=crime)
AICc(model)
summary(model)$adj.r.squared
cat("lambda=0.05\n")
model = lm(Crime~M+So+Ed+Po1+M.F+NW+U2+Ineq+Prob, data=crime)
AICc(model)
summary(model)$adj.r.squared
cat("lambda=0.025\n")
model = lm(Crime~M+So+Ed+Po1+M.F+NW+U1+U2+Wealth+Ineq+Prob, data=crime)
AICc(model)
summary(model)$adj.r.squared
```


## Elastic net  
We set $\alpha=0.5$ in *glmnet* function, so the algorithm is to minimize the objective function: $errors+0.5\lambda(\sum\|\beta_i\|+\sum\beta_i^2)$. Same as Lasso, when the value of $\lambda$ becomes larger, the number of non-zero predictor coefficients decreases and the fraction of deviance explained decreases too.
```{r}
model_elastic = glmnet(as.matrix(crime[,-16]), as.matrix(crime[,16]), 
                     family="mgaussian", alpha=0.5,
                     standardize=TRUE, standardize.response=TRUE)
par(mfrow=c(1,2))
plot(model_elastic$lambda, model_elastic$df, ylab="num of non-zero coefficient")
plot(model_elastic$lambda, model_elastic$dev.ratio, ylab="fraction of deviance explained")
```
  
  
Similarly, we set $\lambda=0.05, 0.1, 0.15, 0.2$ as example and the results showed below (if needed, we can traverse all the lambdas we get). Among these models, we tend to choose Crime~M+So+Ed+Po1+Po2+M.F+NW+Ineq+Prob.  
  
Another thing that is valuable to pay attention is that compare to Lasso, Elastic Net is not that sufficient to exclude some of the highly correlated predictors. As we know, Po1 and Po2 have correlation approximating to 1. In Lasso, only Po1 is included but here, the two variables are included.
```{r}
model_elastic_new = glmnet(as.matrix(crime[,-16]), as.matrix(crime[,16]), 
                     family="mgaussian", alpha=0.5, lambda=c(0.05,0.1,0.15,0.2),
                     standardize=TRUE, standardize.response=TRUE)
model_elastic_new$beta
cat("lambda=0.2\n")
model = lm(Crime~M+Po1+Po2+M.F+Ineq+Prob, data=crime)
AICc(model)
summary(model)$adj.r.squared
cat("lambda=0.15\n")
model = lm(Crime~M+So+Ed+Po1+Po2+M.F+NW+Ineq+Prob, data=crime)
AICc(model)
summary(model)$adj.r.squared
cat("lambda=0.1\n")
model = lm(Crime~M+So+Ed+Po1+Po2+LF+M.F+NW+U2+Ineq+Prob, data=crime)
AICc(model)
summary(model)$adj.r.squared
cat("lambda=0.05\n")
model = lm(Crime~M+So+Ed+Po1+Po2+LF+M.F+NW+U1+U2+Wealth+Ineq+Prob, data=crime)
AICc(model)
summary(model)$adj.r.squared
```

