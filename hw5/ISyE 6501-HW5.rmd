---
title: "ISyE 6501-HOMEWORK 5"
output:
  pdf_document:
    extra_dependencies: ["xcolor"]
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(prompt = TRUE)
        knitr::opts_chunk$set(comment = NA)
        knitr::opts_chunk$set(fig.height = 5)
        knitr::opts_chunk$set(fig.width = 7)
        knitr::opts_chunk$set(fig.align = "center")
```
  
**Group Members:**  
Jinyu, Huang  |  jhuang472@gatech.edu  |  GTID: 903522245  
Chengqi, Huang  |  chengqihuang@gatech.edu  |  GTID: 903534690  
Yuefan, Hu  |  yuefanhu@gatech.edu  |  GTID:903543027  
Jingyu, Li  |  alanli@gatech.edu  |  GTID: 903520148  
  
  
## Qusetion 8.1  
**Describe a situation or problem from your job, everyday life, current events, etc., for which a linear regression model would be appropriate. List some (up to 5) predictors that you might use.**  
\textcolor[RGB]{135,35,28}{Answer}  
In Organizational Psychology, linear regression models are widely used to research on how employees' psychological states influence their behavior in company. For example, to explore what factors will influence employees' KPI performance, we can measure their job satisfaction, organization commitment,  fairness perception as predictors, and then use the KPI performance in a later time point as dependent varible.  
  
  
  
## Qusetion 8.2  
**Using crime data, use regression (a useful R function is lm or glm) to predict the observed crime rate in a city with the following data **  
\textcolor[RGB]{135,35,28}{Answer}  
Considering that we don't have any theoretical knowledge to decide which predictors we should choose, our first step is selecting all the predictors and using "simultaneous" enter method. Because the sample size is relatively small, we get a good fitting model with adjusted $R^2=0.709$ as expected. Given all other predictors in the model, *M, Ed, Ineq, Prob* are statistically significantly associated to *Crime*; and *Po1, U2* are marginally significant. It's hard to do validation to select among different models in such a small data set. As our goal is to make prediction based on given datas on the independent value, we choose to record the residual standard error and AICc of different models for further comparison.
```{r}
library(MuMIn)
crime = read.table("uscrime.txt", header=TRUE)  # import data
model_1 = lm(Crime~., data=crime)
summary(model_1)
quality = matrix(nrow=4, ncol=3)  # store the quality matrix
colnames(quality) = c("model", "residual standard error", "AICc")
quality[1,1] = "m1: Crime~."
quality[1,2] = round(summary(model_1)$sigma,3)
quality[1,3] = round(AICc(model_1),3)
```
  
  
The second model we try only includes the significant predictors in model_1, which are *M, Ed, Po1, U2, Ineq, Prob*. The summary table shows all the predictors in this model are significant and the adjusted $R^2$ of the overall model equals to 0.731. 
```{r}
model_2 = lm(Crime~M+Ed+Po1+U2+Ineq+Prob, data=crime)
summary(model_2)
quality[2,1] = "m2: Crime~M+Ed+Po1+U2+Ineq+Prob"
quality[2,2] = round(summary(model_2)$sigma,3)
quality[2,3] = round(AICc(model_2),3)
```
  
  
Furthermore, we try the stepwise method, which iteratively adds and removes predictors from the model to find a subset of variables resulting in the lowest predicting error. The general function of stepwise method for regression is in library "MASS" and called *stepAIC*. Because our sample size is small and AICc would be a better indicator, we used a modified version called *stepAICc*(https://stat.ethz.ch/pipermail/r-help/2009-April/389888.html). We use the full model (including all predictors) as initial model and choose stepwise method. Results show some top models according to AICc values.
```{r, include=FALSE}
stepAICc <- function (object, scope, scale = 0, direction = c("both", "backward", 
  "forward"), trace = 1, keep = NULL, steps = 1000, use.start = FALSE, 
  k = 2, ...) 
{
  mydeviance <- function(x, ...) {
    dev <- deviance(x)
    if (!is.null(dev)) 
      dev
    else MuMIn::AICc(x, k=0)
  }
  cut.string <- function(string) {
    if (length(string) > 1L) 
      string[-1L] <- paste("\n", string[-1L], sep = "")
    string
  }
  re.arrange <- function(keep) {
    namr <- names(k1 <- keep[[1L]])
    namc <- names(keep)
    nc <- length(keep)
    nr <- length(k1)
    array(unlist(keep, recursive = FALSE), c(nr, nc), list(namr, 
      namc))
  }
  step.results <- function(models, fit, object, usingCp = FALSE) {
    change <- sapply(models, "[[", "change")
    rd <- sapply(models, "[[", "deviance")
    dd <- c(NA, abs(diff(rd)))
    rdf <- sapply(models, "[[", "df.resid")
    ddf <- c(NA, abs(diff(rdf)))
    AIC <- sapply(models, "[[", "AIC")
    heading <- c("Stepwise Model Path \nAnalysis of Deviance Table", 
      "\nInitial Model:", deparse(formula(object)), "\nFinal Model:", 
      deparse(formula(fit)), "\n")
    aod <- if (usingCp) 
      data.frame(Step = change, Df = ddf, Deviance = dd, 
        `Resid. Df` = rdf, `Resid. Dev` = rd, Cp = AIC, 
        check.names = FALSE)
    else data.frame(Step = change, Df = ddf, Deviance = dd, 
      `Resid. Df` = rdf, `Resid. Dev` = rd, AIC = AIC, 
      check.names = FALSE)
    attr(aod, "heading") <- heading
    class(aod) <- c("Anova", "data.frame")
    fit$anova <- aod
    fit
  }
  Terms <- terms(object)
  object$formula <- Terms
  if (inherits(object, "lme")) 
    object$call$fixed <- Terms
  else if (inherits(object, "gls")) 
    object$call$model <- Terms
  else object$call$formula <- Terms
  if (use.start) 
    warning("'use.start' cannot be used with R's version of 'glm'")
  md <- missing(direction)
  direction <- match.arg(direction)
  backward <- direction == "both" | direction == "backward"
  forward <- direction == "both" | direction == "forward"
  if (missing(scope)) {
    fdrop <- numeric()
    fadd <- attr(Terms, "factors")
    if (md) 
      forward <- FALSE
  }
  else {
    if (is.list(scope)) {
      fdrop <- if (!is.null(fdrop <- scope$lower)) 
        attr(terms(update.formula(object, fdrop)), "factors")
      else numeric()
      fadd <- if (!is.null(fadd <- scope$upper)) 
        attr(terms(update.formula(object, fadd)), "factors")
    }
    else {
      fadd <- if (!is.null(fadd <- scope)) 
        attr(terms(update.formula(object, scope)), "factors")
      fdrop <- numeric()
    }
  }
  models <- vector("list", steps)
  if (!is.null(keep)) 
    keep.list <- vector("list", steps)
  n <- nobs(object, use.fallback = TRUE)
  fit <- object
  bAIC <- extractAIC(fit, scale, k = k, ...)
  edf <- bAIC[1L]
  bAIC <- MuMIn::AICc(fit, k=k)
  if (is.na(bAIC)) 
    stop("AIC is not defined for this model, so 'stepAIC' cannot proceed")
  if (bAIC == -Inf) 
    stop("AIC is -infinity for this model, so 'stepAIC' cannot proceed")
  nm <- 1
  Terms <- terms(fit)
  if (trace) {
    cat("Start:  AIC=", format(round(bAIC, 2)), "\n", cut.string(deparse(formula(fit))), 
      "\n\n", sep = "")
    utils::flush.console()
  }
  models[[nm]] <- list(deviance = mydeviance(fit), df.resid = n - 
    edf, change = "", AIC = bAIC)
  if (!is.null(keep)) 
    keep.list[[nm]] <- keep(fit, bAIC)
  usingCp <- FALSE
  while (steps > 0) {
    steps <- steps - 1
    AIC <- bAIC
    ffac <- attr(Terms, "factors")
    if (!is.null(sp <- attr(Terms, "specials")) && !is.null(st <- sp$strata)) 
      ffac <- ffac[-st, ]
    scope <- factor.scope(ffac, list(add = fadd, drop = fdrop))
    aod <- NULL
    change <- NULL
    if (backward && length(scope$drop)) {
      aod <- dropterm(fit, scope$drop, scale = scale, 
        trace = max(0, trace - 1), k = k, ...)
      rn <- row.names(aod)
      row.names(aod) <- c(rn[1L], paste("-", rn[-1L], 
        sep = " "))
      if (any(aod$Df == 0, na.rm = TRUE)) {
        zdf <- aod$Df == 0 & !is.na(aod$Df)
        nc <- match(c("Cp", "AIC"), names(aod))
        nc <- nc[!is.na(nc)][1L]
        ch <- abs(aod[zdf, nc] - aod[1, nc]) > 0.01
        if (any(is.finite(ch) & ch)) {
          warning("0 df terms are changing AIC")
          zdf <- zdf[!ch]
        }
        if (length(zdf) > 0L) 
          change <- rev(rownames(aod)[zdf])[1L]
      }
    }
    if (is.null(change)) {
      if (forward && length(scope$add)) {
        aodf <- addterm(fit, scope$add, scale = scale, 
          trace = max(0, trace - 1), k = k, ...)
        rn <- row.names(aodf)
        row.names(aodf) <- c(rn[1L], paste("+", rn[-1L], 
          sep = " "))
        aod <- if (is.null(aod)) 
          aodf
        else rbind(aod, aodf[-1, , drop = FALSE])
      }
      attr(aod, "heading") <- NULL
      if (is.null(aod) || ncol(aod) == 0) 
        break
      nzdf <- if (!is.null(aod$Df)) 
        aod$Df != 0 | is.na(aod$Df)
      aod <- aod[nzdf, ]
      if (is.null(aod) || ncol(aod) == 0) 
        break
      nc <- match(c("Cp", "AIC"), names(aod))
      nc <- nc[!is.na(nc)][1L]
      o <- order(aod[, nc])
      if (trace) {
        print(aod[o, ])
        utils::flush.console()
      }
      if (o[1L] == 1) 
        break
      change <- rownames(aod)[o[1L]]
    }
    usingCp <- match("Cp", names(aod), 0) > 0
    fit <- update(fit, paste("~ .", change), evaluate = FALSE)
    fit <- eval.parent(fit)
    nnew <- nobs(fit, use.fallback = TRUE)
    if (all(is.finite(c(n, nnew))) && nnew != n) 
      stop("number of rows in use has changed: remove missing values?")
    Terms <- terms(fit)
    bAIC <- extractAIC(fit, scale, k = k, ...)
    edf <- bAIC[1L]
    bAIC <- MuMIn::AICc(fit, k=k)
    if (trace) {
      cat("\nStep:  AIC=", format(round(bAIC, 2)), "\n", 
        cut.string(deparse(formula(fit))), "\n\n", sep = "")
      utils::flush.console()
    }
    if (bAIC >= AIC + 1e-07) 
      break
    nm <- nm + 1
    models[[nm]] <- list(deviance = mydeviance(fit), df.resid = n - 
      edf, change = change, AIC = bAIC)
    if (!is.null(keep)) 
      keep.list[[nm]] <- keep(fit, bAIC)
  }
  if (!is.null(keep)) 
    fit$keep <- re.arrange(keep.list[seq(nm)])
  step.results(models = models[seq(nm)], fit, object, usingCp)
}
```

```{r}
library(MASS)
full.model = lm(Crime~., data=crime)
stepAICc(full.model, direction = "both", steps=2000) 
```
  
  
So we choose the top 2 models from stepwise method and add them into comparison. Based on residual standard error and AICc, *model_2* and *model_3* are two better ones. Because AICc contains penalty term to balance likelihood with simplicity, we can see the more simple model(*model_2*) has a lower AICc but higher residual standard error. We propose that lower residual standard error is more important in this question for doing prediction and the complexity of *model_3* and *model_2* differs not much. So we finally choose *model_3*.
```{r}
model_3 = lm(Crime~M+Ed+Po1+M.F+U1+U2+Ineq+Prob, data=crime)
quality[3,1] = "m3: Crime~M+Ed+Po1+M.F+U1+U2+Ineq+Prob"
quality[3,2] = round(summary(model_3)$sigma,3)
quality[3,3] = round(AICc(model_3),3)

model_4 = lm(Crime~M+Ed+Po1+M.F+U1+U2+Wealth+Ineq+Prob, data=crime)
quality[4,1] = "m4: Crime~M+Ed+Po1+M.F+U1+U2+Wealth+Ineq+Prob"
quality[4,2] = round(summary(model_4)$sigma,3)
quality[4,3] = round(AICc(model_4),3)


quality
```
  
  
As for model_3, the adjusted $R^2$ is 0.744 and overall F-value is 17.74 with $p \approx 0$. The regression model is:
$Crime=-6426.10+93.32M+180.12Ed+102.65PO1+22.34M.F-6086.63U1+187.35U2+61.33Ineq+-3796.03Prob$
```{r}
summary(model_3)
```
  
  
Then we use model_3 to do prediction based on the given data. We apply the *predict* function and use 0.99 confidence level. With the new given data, the predicted crime is 1038 and the 99% confidence interval is $(376.41,1700.41)$.
```{r}
target_data = data.frame(M=14, 
                         Ed=10, 
                         Po1=12, 
                         M.F=94, 
                         U1=0.12, 
                         U2=3.6, 
                         Ineq=20.1, 
                         Prob=0.04)
predict(model_3, target_data, interval="predict", level=0.99) 
```

